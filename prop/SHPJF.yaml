# Data
min_word_cnt: 5
geek_longsent_len: 0
job_longsent_len: 192
query_his_len: 64
query_wd_len: 4

# Model
wd_embedding_size: 128
user_embedding_size: 16
bert_embedding_size: 768
hidden_size: 64
dropout: 0.2
num_heads: 1
beta: 0.2
k: 8

# Training
learning_rate: 0.0003

# General
train_batch_size: 2048
eval_batch_size: 4096
